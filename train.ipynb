{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-md==3.7.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.7.1/en_core_web_md-3.7.1-py3-none-any.whl (42.8 MB)\n",
      "     ---------------------------------------- 0.0/42.8 MB ? eta -:--:--\n",
      "     --------------------------------------- 0.0/42.8 MB 330.3 kB/s eta 0:02:10\n",
      "     --------------------------------------- 0.0/42.8 MB 325.1 kB/s eta 0:02:12\n",
      "     --------------------------------------- 0.0/42.8 MB 325.1 kB/s eta 0:02:12\n",
      "     --------------------------------------- 0.0/42.8 MB 217.9 kB/s eta 0:03:17\n",
      "     --------------------------------------- 0.1/42.8 MB 374.1 kB/s eta 0:01:55\n",
      "     --------------------------------------- 0.1/42.8 MB 437.6 kB/s eta 0:01:38\n",
      "     --------------------------------------- 0.1/42.8 MB 379.3 kB/s eta 0:01:53\n",
      "     --------------------------------------- 0.2/42.8 MB 513.4 kB/s eta 0:01:23\n",
      "     --------------------------------------- 0.2/42.8 MB 628.1 kB/s eta 0:01:08\n",
      "     --------------------------------------- 0.3/42.8 MB 561.1 kB/s eta 0:01:16\n",
      "     --------------------------------------- 0.3/42.8 MB 613.6 kB/s eta 0:01:10\n",
      "     --------------------------------------- 0.4/42.8 MB 675.0 kB/s eta 0:01:03\n",
      "     --------------------------------------- 0.4/42.8 MB 655.5 kB/s eta 0:01:05\n",
      "     --------------------------------------- 0.4/42.8 MB 655.4 kB/s eta 0:01:05\n",
      "      -------------------------------------- 0.6/42.8 MB 838.1 kB/s eta 0:00:51\n",
      "      -------------------------------------- 0.6/42.8 MB 907.9 kB/s eta 0:00:47\n",
      "      -------------------------------------- 0.7/42.8 MB 962.5 kB/s eta 0:00:44\n",
      "      --------------------------------------- 0.8/42.8 MB 1.0 MB/s eta 0:00:41\n",
      "     - -------------------------------------- 1.2/42.8 MB 1.4 MB/s eta 0:00:31\n",
      "     - -------------------------------------- 1.4/42.8 MB 1.5 MB/s eta 0:00:28\n",
      "     - -------------------------------------- 1.5/42.8 MB 1.6 MB/s eta 0:00:26\n",
      "     - -------------------------------------- 2.1/42.8 MB 2.1 MB/s eta 0:00:19\n",
      "     -- ------------------------------------- 2.6/42.8 MB 2.5 MB/s eta 0:00:17\n",
      "     -- ------------------------------------- 3.0/42.8 MB 2.8 MB/s eta 0:00:15\n",
      "     --- ------------------------------------ 3.9/42.8 MB 3.4 MB/s eta 0:00:12\n",
      "     ---- ----------------------------------- 4.8/42.8 MB 4.1 MB/s eta 0:00:10\n",
      "     ----- ---------------------------------- 5.9/42.8 MB 4.9 MB/s eta 0:00:08\n",
      "     ------ --------------------------------- 6.9/42.8 MB 5.4 MB/s eta 0:00:07\n",
      "     -------- ------------------------------- 8.6/42.8 MB 6.5 MB/s eta 0:00:06\n",
      "     -------- ------------------------------- 8.6/42.8 MB 6.5 MB/s eta 0:00:06\n",
      "     -------- ------------------------------- 8.6/42.8 MB 6.5 MB/s eta 0:00:06\n",
      "     -------- ------------------------------- 8.6/42.8 MB 6.5 MB/s eta 0:00:06\n",
      "     -------- ------------------------------- 8.6/42.8 MB 5.9 MB/s eta 0:00:06\n",
      "     -------- ------------------------------- 8.6/42.8 MB 5.9 MB/s eta 0:00:06\n",
      "     -------- ------------------------------- 8.6/42.8 MB 5.9 MB/s eta 0:00:06\n",
      "     -------- ------------------------------- 8.6/42.8 MB 5.9 MB/s eta 0:00:06\n",
      "     --------- ------------------------------ 9.7/42.8 MB 5.8 MB/s eta 0:00:06\n",
      "     ----------- --------------------------- 13.0/42.8 MB 15.2 MB/s eta 0:00:02\n",
      "     ------------ -------------------------- 13.7/42.8 MB 16.8 MB/s eta 0:00:02\n",
      "     ------------- ------------------------- 15.0/42.8 MB 16.8 MB/s eta 0:00:02\n",
      "     ------------- ------------------------- 15.0/42.8 MB 16.8 MB/s eta 0:00:02\n",
      "     ------------- ------------------------- 15.0/42.8 MB 16.8 MB/s eta 0:00:02\n",
      "     ------------- ------------------------- 15.0/42.8 MB 16.8 MB/s eta 0:00:02\n",
      "     ----------------- --------------------- 19.4/42.8 MB 32.8 MB/s eta 0:00:01\n",
      "     -------------------- ------------------ 22.6/42.8 MB 31.2 MB/s eta 0:00:01\n",
      "     -------------------- ------------------ 22.6/42.8 MB 31.2 MB/s eta 0:00:01\n",
      "     --------------------- ----------------- 24.0/42.8 MB 28.4 MB/s eta 0:00:01\n",
      "     ----------------------- --------------- 25.3/42.8 MB 54.4 MB/s eta 0:00:01\n",
      "     ----------------------- --------------- 25.3/42.8 MB 54.4 MB/s eta 0:00:01\n",
      "     ----------------------- --------------- 25.3/42.8 MB 54.4 MB/s eta 0:00:01\n",
      "     ------------------------ -------------- 26.4/42.8 MB 31.2 MB/s eta 0:00:01\n",
      "     ------------------------- ------------- 27.4/42.8 MB 26.2 MB/s eta 0:00:01\n",
      "     --------------------------- ----------- 29.9/42.8 MB 25.2 MB/s eta 0:00:01\n",
      "     --------------------------- ----------- 30.6/42.8 MB 23.4 MB/s eta 0:00:01\n",
      "     ---------------------------- ---------- 31.4/42.8 MB 22.6 MB/s eta 0:00:01\n",
      "     ----------------------------- --------- 32.5/42.8 MB 19.8 MB/s eta 0:00:01\n",
      "     ------------------------------ -------- 33.3/42.8 MB 22.6 MB/s eta 0:00:01\n",
      "     ------------------------------- ------- 34.3/42.8 MB 21.1 MB/s eta 0:00:01\n",
      "     ------------------------------- ------- 34.6/42.8 MB 19.9 MB/s eta 0:00:01\n",
      "     ------------------------------- ------- 34.6/42.8 MB 19.9 MB/s eta 0:00:01\n",
      "     ------------------------------- ------- 34.6/42.8 MB 19.9 MB/s eta 0:00:01\n",
      "     ------------------------------- ------- 34.6/42.8 MB 19.9 MB/s eta 0:00:01\n",
      "     ---------------------------------- ---- 38.2/42.8 MB 20.5 MB/s eta 0:00:01\n",
      "     ----------------------------------- --- 38.8/42.8 MB 19.8 MB/s eta 0:00:01\n",
      "     ----------------------------------- --- 38.8/42.8 MB 19.8 MB/s eta 0:00:01\n",
      "     ----------------------------------- --- 38.8/42.8 MB 19.8 MB/s eta 0:00:01\n",
      "     ----------------------------------- --- 38.8/42.8 MB 19.8 MB/s eta 0:00:01\n",
      "     ----------------------------------- --- 39.3/42.8 MB 14.9 MB/s eta 0:00:01\n",
      "     ------------------------------------- - 40.8/42.8 MB 16.0 MB/s eta 0:00:01\n",
      "     ------------------------------------- - 40.8/42.8 MB 16.0 MB/s eta 0:00:01\n",
      "     ------------------------------------- - 41.6/42.8 MB 13.6 MB/s eta 0:00:01\n",
      "     --------------------------------------  42.8/42.8 MB 14.6 MB/s eta 0:00:01\n",
      "     --------------------------------------  42.8/42.8 MB 14.6 MB/s eta 0:00:01\n",
      "     --------------------------------------  42.8/42.8 MB 14.6 MB/s eta 0:00:01\n",
      "     --------------------------------------  42.8/42.8 MB 14.6 MB/s eta 0:00:01\n",
      "     --------------------------------------  42.8/42.8 MB 14.6 MB/s eta 0:00:01\n",
      "     --------------------------------------  42.8/42.8 MB 14.6 MB/s eta 0:00:01\n",
      "     --------------------------------------  42.8/42.8 MB 14.6 MB/s eta 0:00:01\n",
      "     --------------------------------------  42.8/42.8 MB 14.6 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 42.8/42.8 MB 9.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: spacy<3.8.0,>=3.7.2 in c:\\users\\chuchung\\dev\\model\\lib\\site-packages (from en-core-web-md==3.7.1) (3.7.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\chuchung\\dev\\model\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\chuchung\\dev\\model\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\chuchung\\dev\\model\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\chuchung\\dev\\model\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\chuchung\\dev\\model\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in c:\\users\\chuchung\\dev\\model\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (8.2.1)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\chuchung\\dev\\model\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\chuchung\\dev\\model\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\chuchung\\dev\\model\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in c:\\users\\chuchung\\dev\\model\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.3.4)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in c:\\users\\chuchung\\dev\\model\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.9.0)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\chuchung\\dev\\model\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (6.4.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\chuchung\\dev\\model\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (4.66.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\chuchung\\dev\\model\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\chuchung\\dev\\model\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.5.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\chuchung\\dev\\model\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.1.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\chuchung\\dev\\model\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (69.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\chuchung\\dev\\model\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (23.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\chuchung\\dev\\model\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.3.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\chuchung\\dev\\model\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.26.2)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\chuchung\\dev\\model\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.14.5 in c:\\users\\chuchung\\dev\\model\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.14.5)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\chuchung\\dev\\model\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (4.8.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\chuchung\\dev\\model\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\chuchung\\dev\\model\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\chuchung\\dev\\model\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\chuchung\\dev\\model\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2023.11.17)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\chuchung\\dev\\model\\lib\\site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\chuchung\\dev\\model\\lib\\site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.1.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\chuchung\\dev\\model\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\chuchung\\dev\\model\\lib\\site-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (8.1.7)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in c:\\users\\chuchung\\dev\\model\\lib\\site-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\chuchung\\dev\\model\\lib\\site-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.1.3)\n",
      "Installing collected packages: en-core-web-md\n",
      "Successfully installed en-core-web-md-3.7.1\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_md')\n"
     ]
    }
   ],
   "source": [
    "#!python -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>tag</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I’m looking for datasets or api source that quantifies fan base, or preferably, bettors’ sentiment regarding a team’s performance or direction. Does anyone know of an API that tracks this? For now I’m looking specifically for NBA, but am also interested in MLB, NFL, and NCAA f-ball and b-ball.</td>\n",
       "      <td>API</td>\n",
       "      <td>datasets</td>\n",
       "      <td>s0vufk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I'm making an ESG stock analysis program in Java, and so far the only free ESG API I've come across is ESGEnterprise, but I'm having trouble retrieving the data. Has anyone had any success/have any recs for other ESG APIs out there.</td>\n",
       "      <td>API</td>\n",
       "      <td>datasets</td>\n",
       "      <td>ruvj9n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hey everyone! I’m one of the creators of Sieve _URL_ and I’m excited to be sharing it!\\nSieve _URL_ **is an API that helps you turn petabyte-scale video data into a high-quality dataset, automatically.**\\nIt helps store, process, and semantically search your video data. Just think _NUMBER_ cameras recording footage at _NUMBER_ FPS, _NUMBER_/_NUMBER_. That would be _NUMBER_ million frames generated in a single day. The videos might be searchable by timestamp, but finding moments of interest is like searching for a needle in a haystack. Sieve tags useful attributes like people, motion, lighting, etc on every frame!\\nWe built this visual demo (link here _URL_ a little while back which we’d love to get feedback on. It’s \\~_NUMBER_ hours of security footage that our API processed in &lt;_NUMBER_ mins and has simple querying and export functionality enabled. We see applications in better understanding what data you have, figuring out which data to send to labeling, sampling datasets for training, and building multiple test sets for models by scenario.\\nTo try it on your videos: _URL_ _URL_\\nVisual dashboard walkthrough: Click on our site link!</td>\n",
       "      <td>API</td>\n",
       "      <td>datasets</td>\n",
       "      <td>rup1uj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>718</th>\n",
       "      <td>I'm currently in the process of learning NLP. I am using catalyst on c#. \\nI was able to run the sample programs and it was able to determine if the word is an noun, adjective, etc. But I can't find any sample for what I need.\\nHere is a summary of what I would like to achieve. \\nI would like to extract certain information on a sentence. Lets say i have the following texts:\\n\"Sally ate an orange this morning. \"\\nOr \\n\"Sally is hiding behind the cabinet and she is eating an orange. \" \\nHow do i use the nlp to extract what sally ate?</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LanguageTechnology</td>\n",
       "      <td>saas64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>719</th>\n",
       "      <td>I’ve been trying to do some basic keyword extraction and finding it harder than expected.\\nKeyBERT seems good but it requires a powerful GPU to be usably fast. That’s possible with AWS, but there’s a bit more set up.\\nI just tried PyTextRank, and I was surprised at the quality of the output - I wouldn’t say it was perfect either. Maybe I should set a threshold, like choose the top _NUMBER_ ranked keywords? It’s fine if we exclude potential good keywords just to have a smaller list of good ones.\\nHere’s a good article about _NUMBER_ different methods, which is helpful -\\n_URL_\\nIn theory, Spacy and BERT seem like the best options but they’re both a little complex. \\nI think KW extraction really only needs a few layers or as Spacy would call them pipelines.\\n_NUMBER_. accurate tokenization of words and punctuation symbols\\n_NUMBER_. accurate recognition of multi-word expressions  - think of it as “chunking”\\n_NUMBER_. Strong assessment of keyword “candidacy” for each MWE \\nOf course, a good algorithm can often skip steps. Like BERT is so smart it doesn’t need anything but the input text.\\nDoes anyone know of a simplest way to run a fast, effective keyword extraction?\\nI’m talking _NUMBER_ keywords in one second on a fast CPU.\\nThanks very much</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LanguageTechnology</td>\n",
       "      <td>s7qml8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>720</th>\n",
       "      <td>This _URL_ position is currently open and I wanted to share with you!</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LanguageTechnology</td>\n",
       "      <td>s30ccv</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>721 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              text  \\\n",
       "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           I’m looking for datasets or api source that quantifies fan base, or preferably, bettors’ sentiment regarding a team’s performance or direction. Does anyone know of an API that tracks this? For now I’m looking specifically for NBA, but am also interested in MLB, NFL, and NCAA f-ball and b-ball.   \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         I'm making an ESG stock analysis program in Java, and so far the only free ESG API I've come across is ESGEnterprise, but I'm having trouble retrieving the data. Has anyone had any success/have any recs for other ESG APIs out there.   \n",
       "2                                                                                                                 Hey everyone! I’m one of the creators of Sieve _URL_ and I’m excited to be sharing it!\\nSieve _URL_ **is an API that helps you turn petabyte-scale video data into a high-quality dataset, automatically.**\\nIt helps store, process, and semantically search your video data. Just think _NUMBER_ cameras recording footage at _NUMBER_ FPS, _NUMBER_/_NUMBER_. That would be _NUMBER_ million frames generated in a single day. The videos might be searchable by timestamp, but finding moments of interest is like searching for a needle in a haystack. Sieve tags useful attributes like people, motion, lighting, etc on every frame!\\nWe built this visual demo (link here _URL_ a little while back which we’d love to get feedback on. It’s \\~_NUMBER_ hours of security footage that our API processed in <_NUMBER_ mins and has simple querying and export functionality enabled. We see applications in better understanding what data you have, figuring out which data to send to labeling, sampling datasets for training, and building multiple test sets for models by scenario.\\nTo try it on your videos: _URL_ _URL_\\nVisual dashboard walkthrough: Click on our site link!   \n",
       "..                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             ...   \n",
       "718                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      I'm currently in the process of learning NLP. I am using catalyst on c#. \\nI was able to run the sample programs and it was able to determine if the word is an noun, adjective, etc. But I can't find any sample for what I need.\\nHere is a summary of what I would like to achieve. \\nI would like to extract certain information on a sentence. Lets say i have the following texts:\\n\"Sally ate an orange this morning. \"\\nOr \\n\"Sally is hiding behind the cabinet and she is eating an orange. \" \\nHow do i use the nlp to extract what sally ate?   \n",
       "719  I’ve been trying to do some basic keyword extraction and finding it harder than expected.\\nKeyBERT seems good but it requires a powerful GPU to be usably fast. That’s possible with AWS, but there’s a bit more set up.\\nI just tried PyTextRank, and I was surprised at the quality of the output - I wouldn’t say it was perfect either. Maybe I should set a threshold, like choose the top _NUMBER_ ranked keywords? It’s fine if we exclude potential good keywords just to have a smaller list of good ones.\\nHere’s a good article about _NUMBER_ different methods, which is helpful -\\n_URL_\\nIn theory, Spacy and BERT seem like the best options but they’re both a little complex. \\nI think KW extraction really only needs a few layers or as Spacy would call them pipelines.\\n_NUMBER_. accurate tokenization of words and punctuation symbols\\n_NUMBER_. accurate recognition of multi-word expressions  - think of it as “chunking”\\n_NUMBER_. Strong assessment of keyword “candidacy” for each MWE \\nOf course, a good algorithm can often skip steps. Like BERT is so smart it doesn’t need anything but the input text.\\nDoes anyone know of a simplest way to run a fast, effective keyword extraction?\\nI’m talking _NUMBER_ keywords in one second on a fast CPU.\\nThanks very much   \n",
       "720                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          This _URL_ position is currently open and I wanted to share with you!   \n",
       "\n",
       "     tag           subreddit      id  \n",
       "0    API            datasets  s0vufk  \n",
       "1    API            datasets  ruvj9n  \n",
       "2    API            datasets  rup1uj  \n",
       "..   ...                 ...     ...  \n",
       "718  NaN  LanguageTechnology  saas64  \n",
       "719  NaN  LanguageTechnology  s7qml8  \n",
       "720  NaN  LanguageTechnology  s30ccv  \n",
       "\n",
       "[721 rows x 4 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.options.display.max_colwidth = None\n",
    "pd.options.display.max_rows = 6\n",
    "data = pd.read_csv(\"spacy_textcat/reddit_data.csv\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['datasets', 'dataengineering', 'LanguageTechnology']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#for the text classification component\n",
    "cats = data.subreddit.unique().tolist()\n",
    "cats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Set, List, Tuple\n",
    "from spacy.tokens import DocBin\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "def make_docs(data: List[Tuple[str, str]], target_file: str, cats: Set[str]):\n",
    "    docs = DocBin()\n",
    "    # Use nlp.pipe to efficiently process a large number of text inputs,\n",
    "    # the as_tuple arguments enables giving a list of tuples as input and\n",
    "    # reuse it in the loop, here for the labels\n",
    "    for doc, label in nlp.pipe(data, as_tuples=True):\n",
    "        # Encode the labels (assign 1 the subreddit)\n",
    "        for cat in cats:\n",
    "            doc.cats[cat] = 1 if cat == label else 0\n",
    "        docs.add(doc)\n",
    "    docs.to_disk(target_file)\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "504 504\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(data[\"text\"].values, data[\"subreddit\"].values, test_size=0.3)\n",
    "\n",
    "# make_docs(list(zip(X_train, y_train)), \"train.spacy\", cats=cats)\n",
    "# make_docs(list(zip(X_valid, y_valid)), \"valid.spacy\", cats=cats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Created output directory: output\\spacy_textcat\u001b[0m\n",
      "\u001b[38;5;4mℹ Saving to output directory: output\\spacy_textcat\u001b[0m\n",
      "\u001b[38;5;4mℹ Using CPU\u001b[0m\n",
      "\u001b[1m\n",
      "=========================== Initializing pipeline ===========================\u001b[0m\n",
      "\u001b[38;5;2m✔ Initialized pipeline\u001b[0m\n",
      "\u001b[1m\n",
      "============================= Training pipeline =============================\u001b[0m\n",
      "\u001b[38;5;4mℹ Pipeline: ['textcat']\u001b[0m\n",
      "\u001b[38;5;4mℹ Initial learn rate: 0.001\u001b[0m\n",
      "E    #       LOSS TEXTCAT  CATS_SCORE  SCORE \n",
      "---  ------  ------------  ----------  ------\n",
      "  0       0          0.22        3.59    0.04\n",
      "  0     200         37.56       37.31    0.37\n",
      "  0     400         26.53       55.96    0.56\n",
      "  1     600          7.89       74.00    0.74\n",
      "  1     800          9.21       79.53    0.80\n",
      "  2    1000          2.57       83.73    0.84\n",
      "  3    1200          1.48       75.15    0.75\n",
      "  4    1400          1.82       80.75    0.81\n",
      "  5    1600          0.63       81.07    0.81\n",
      "  6    1800          0.91       79.96    0.80\n",
      "  7    2000          0.53       79.91    0.80\n",
      "  9    2200          0.52       81.68    0.82\n",
      " 11    2400          0.62       79.89    0.80\n",
      " 14    2600          0.51       79.40    0.79\n",
      "\u001b[38;5;2m✔ Saved pipeline to output directory\u001b[0m\n",
      "output\\spacy_textcat\\model-last\n"
     ]
    }
   ],
   "source": [
    "from spacy.cli.train import train as spacy_train\n",
    "config_path = \"spacy_textcat/config.cfg\"\n",
    "output_model_path = \"output/spacy_textcat\"\n",
    "spacy_train(\n",
    "    config_path,\n",
    "    output_path=output_model_path,\n",
    "    overrides={\n",
    "        \"paths.train\": \"train.spacy\",\n",
    "        \"paths.dev\": \"valid.spacy\",\n",
    "    },\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "model",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
